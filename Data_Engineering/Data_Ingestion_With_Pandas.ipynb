{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e89e400",
   "metadata": {},
   "source": [
    "# Streamlined Data Ingestion With Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c9fc3c",
   "metadata": {},
   "source": [
    "## Flat Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428d0322",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#! DO NOT RUN\n",
    "# Create dataframe of next 500 rows with labeled columns\n",
    "vt_data_next500 = pd.read_csv(\"vt_tax_data_2016.csv\", \n",
    "                       \t\t  nrows = 500,\n",
    "                       \t\t  skiprows = 500,\n",
    "                       \t\t  header = None,\n",
    "                       \t\t  names = vt_data_first500.columns)\n",
    "\n",
    "# View the Vermont dataframes to confirm they're different\n",
    "print(vt_data_first500.head())\n",
    "print(vt_data_next500.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909c9c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dict specifying data types for agi_stub and zipcode\n",
    "data_types = {'agi_stub':'category',\n",
    "\t\t\t  'zipcode':'string'}\n",
    "\n",
    "# Load csv using dtype to set correct data types\n",
    "data = pd.read_csv(\"vt_tax_data_2016.csv\", dtype = data_types)\n",
    "\n",
    "# Print data types of resulting frame\n",
    "print(data.dtypes.head())\n",
    "\n",
    "# Create dict specifying that 0s in zipcode are NA values\n",
    "null_values = {\"zipcode\": 0}\n",
    "\n",
    "# Load csv using na_values keyword argument\n",
    "data = pd.read_csv(\"vt_tax_data_2016.csv\", \n",
    "                   na_values = null_values)\n",
    "\n",
    "# View rows with NA ZIP codes\n",
    "print(data[data.zipcode.isna()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3bfbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  # Set warn_bad_lines to issue warnings about bad records\n",
    "  data = pd.read_csv(\"vt_tax_data_2016_corrupt.csv\", \n",
    "                     error_bad_lines=False, \n",
    "                     warn_bad_lines = True)\n",
    "  \n",
    "  # View first 5 records\n",
    "  print(data.head())\n",
    "  \n",
    "except pd.errors.ParserError:\n",
    "    print(\"Your data contained rows that could not be parsed.\")\n",
    "    \n",
    "#* The error_bad_lines and warn_bad_lines are both depreceated. Use on_bad_lines = 'skip','warn' instead"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a033938",
   "metadata": {},
   "source": [
    "## Excel Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93746d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create string of lettered columns to load\n",
    "col_string = 'AD,AW:BA'\n",
    "\n",
    "#* Load data with skiprows and usecols set\n",
    "survey_responses = pd.read_excel(\"fcc_survey_headers.xlsx\", \n",
    "                        skiprows=2, \n",
    "                        usecols = col_string)\n",
    "\n",
    "# View the names of the columns selected\n",
    "print(survey_responses.columns)\n",
    "\n",
    "#* Load all sheets in the Excel file\n",
    "all_survey_data = pd.read_excel(\"fcc_survey.xlsx\",\n",
    "                                sheet_name=None)\n",
    "\n",
    "# View the sheet names in all_survey_data\n",
    "print(all_survey_data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2278fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty dataframe\n",
    "all_responses = pd.DataFrame({})\n",
    "\n",
    "# Set up for loop to iterate through values in responses\n",
    "for df in responses.values():\n",
    "  # Print the number of rows being added\n",
    "  print(\"Adding {} rows\".format(df.shape[0]))\n",
    "  # Concatenate all_responses and df, assign result\n",
    "  all_responses = pd.concat([all_responses, df])\n",
    "\n",
    "# Graph employment statuses in sample\n",
    "counts = all_responses.groupby(\"EmploymentStatus\").EmploymentStatus.count()\n",
    "counts.plot.barh()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1729c8",
   "metadata": {},
   "source": [
    "### Modifying Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910f2d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set dtype to load appropriate column(s) as Boolean data\n",
    "survey_data = pd.read_excel(\"fcc_survey_subset.xlsx\",\n",
    "                            dtype={'HasDebt':'bool'})\n",
    "\n",
    "# View financial burdens by Boolean group\n",
    "print(survey_data.groupby('HasDebt').sum())\n",
    "\n",
    "\n",
    "# Load file, with Part1StartTime parsed as datetime data\n",
    "survey_data = pd.read_excel(\"fcc_survey.xlsx\",\n",
    "                            parse_dates = ['Part1StartTime'])\n",
    "\n",
    "# Print first few values of Part1StartTime\n",
    "print(survey_data.Part1StartTime.head())\n",
    "\n",
    "\n",
    "# Create dict of columns to combine into new datetime column\n",
    "datetime_cols = {\"Part2Start\": ['Part2StartDate','Part2StartTime']}\n",
    "\n",
    "\n",
    "# Load file, supplying the dict to parse_dates\n",
    "survey_data = pd.read_excel(\"fcc_survey_dts.xlsx\",\n",
    "                            parse_dates = datetime_cols)\n",
    "\n",
    "# View summary statistics about Part2Start\n",
    "print(survey_data.Part2Start.describe())\n",
    "\n",
    "\n",
    "# Parse datetimes and assign result back to Part2EndTime\n",
    "survey_data[\"Part2EndTime\"] = pd.to_datetime(survey_data[\"Part2EndTime\"], \n",
    "                                             format=\"%m%d%Y %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcce031a",
   "metadata": {},
   "source": [
    "## Databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea83944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Create the database engine\n",
    "engine = create_engine('sqlite:///data.db')\n",
    "\n",
    "# Load hpd311calls without any SQL\n",
    "hpd_calls = pd.read_sql(\"hpd3calls\", engine)\n",
    "\n",
    "# View the first few rows of data\n",
    "print(hpd_calls.head())\n",
    "\n",
    "\n",
    "#* Using query\n",
    "# Create the database engine\n",
    "engine = create_engine(\"sqlite:///data.db\")\n",
    "\n",
    "# Create a SQL query to load the entire weather table\n",
    "query = \"\"\"\n",
    "SELECT * \n",
    "  FROM weather;\n",
    "\"\"\"\n",
    "\n",
    "# Load weather with the SQL query\n",
    "weather = pd.read_sql(query, engine)\n",
    "\n",
    "# View the first few rows of data\n",
    "print(weather.head()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d14bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create query to get hpd311calls records about safety\n",
    "query = \"\"\"\n",
    "SELECT *\n",
    "FROM hpd311calls\n",
    "WHERE complaint_type == 'SAFETY';\n",
    "\"\"\"\n",
    "\n",
    "# Query the database and assign result to safety_calls\n",
    "safety_calls = pd.read_sql(query,engine)\n",
    "\n",
    "# Graph the number of safety calls by borough\n",
    "call_counts = safety_calls.groupby('borough').unique_key.count()\n",
    "call_counts.plot.barh()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Create query to get call counts by complaint_type\n",
    "query = \"\"\"\n",
    "SELECT complaint_type, \n",
    "     COUNT(*)\n",
    "  FROM hpd311calls\n",
    "  GROUPBY complaint_type;\n",
    "\"\"\"\n",
    "\n",
    "# Create dataframe of call counts by issue\n",
    "calls_by_issue = pd.read_sql(query, engine)\n",
    "\n",
    "# Graph the number of calls for each housing issue\n",
    "calls_by_issue.plot.barh(x=\"complaint_type\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Modify query to join tmax and tmin from weather by date\n",
    "query = \"\"\"\n",
    "SELECT hpd311calls.created_date, \n",
    "\t   COUNT(*), \n",
    "       weather.tmin,\n",
    "       weather.tmax\n",
    "  FROM hpd311calls \n",
    "       JOIN weather\n",
    "       ON hpd311calls.created_date = weather.date\n",
    " WHERE hpd311calls.complaint_type = 'HEAT/HOT WATER' \n",
    " GROUP BY hpd311calls.created_date;\n",
    " \"\"\"\n",
    "\n",
    "# Query database and save results as df\n",
    "df = pd.read_sql(query, engine)\n",
    "\n",
    "# View first 5 records\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cded26a6",
   "metadata": {},
   "source": [
    "## JSON Data and APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e135a5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pandas as pd\n",
    "import pandas as pd\n",
    "\n",
    "# Load the daily report to a dataframe\n",
    "pop_in_shelters = pd.read_json('dhs_daily_report.json',orient='records',typ='frame')\n",
    "\n",
    "# View summary stats about pop_in_shelters\n",
    "print(pop_in_shelters.head())\n",
    "\n",
    "try:\n",
    "    # Load the JSON with orient specified\n",
    "    df = pd.read_json(\"dhs_report_reformatted.json\",\n",
    "                      orient='split',typ='frame')\n",
    "    \n",
    "    # Plot total population in shelters over time\n",
    "    df[\"date_of_census\"] = pd.to_datetime(df[\"date_of_census\"])\n",
    "    df.plot(x=\"date_of_census\", \n",
    "            y=\"total_individuals_in_shelter\")\n",
    "    plt.show()\n",
    "    \n",
    "except ValueError:\n",
    "    print(\"pandas could not parse the JSON.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c20f56",
   "metadata": {},
   "source": [
    "### Working with APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee37f748",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "api_url = \"https://api.yelp.com/v3/businesses/search\"\n",
    "\n",
    "# Get data about NYC cafes from the Yelp API\n",
    "response = requests.get(api_url, \n",
    "                headers=headers, \n",
    "                params=params)\n",
    "\n",
    "# Extract JSON data from the response\n",
    "data = response.json()\n",
    "\n",
    "# Load data to a dataframe\n",
    "cafes = pd.DataFrame(data['businesses'])\n",
    "\n",
    "# View the data's dtypes\n",
    "print(cafes.dtypes)\n",
    "\n",
    "\n",
    "# Create dictionary to query API for cafes in NYC\n",
    "parameters = {\"term\":'cafe',\n",
    "          \t  \"location\":'NYC'}\n",
    "\n",
    "# Query the Yelp API with headers and params set\n",
    "response = requests.get(api_url,\n",
    "                headers=headers,\n",
    "                params=parameters)\n",
    "\n",
    "# Extract JSON data from response\n",
    "data = response.json()\n",
    "\n",
    "# Load \"businesses\" values to a dataframe and print head\n",
    "cafes = pd.DataFrame(data['businesses'])\n",
    "print(cafes.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a749fa99",
   "metadata": {},
   "source": [
    "### Using `pandas.io.json` for Nested JSONs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f33940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load json_normalize()\n",
    "from pandas.io.json import json_normalize\n",
    "\n",
    "# Isolate the JSON data from the API response\n",
    "data = response.json()\n",
    "\n",
    "\n",
    "# Flatten business data into a dataframe, replace separator\n",
    "cafes = json_normalize(data[\"businesses\"],\n",
    "             sep='_')\n",
    "\n",
    "# View data\n",
    "print(cafes.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c06bb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load other business attributes and set meta prefix\n",
    "flat_cafes = json_normalize(data[\"businesses\"],\n",
    "                            sep=\"_\",\n",
    "                    \t\trecord_path=\"categories\",\n",
    "                    \t\tmeta=['name', \n",
    "                                  'alias',  \n",
    "                                  'rating',\n",
    "                          \t\t  ['coordinates', 'latitude'], \n",
    "                          \t\t  ['coordinates', 'longitude']],\n",
    "                    \t\tmeta_prefix=\"biz_\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# View the data\n",
    "print(flat_cafes.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904a91e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add an offset parameter to get cafes 51-100\n",
    "params = {\"term\": \"cafe\", \n",
    "          \"location\": \"NYC\",\n",
    "          \"sort_by\": \"rating\", \n",
    "          \"limit\": 50,\n",
    "          'offset':50}\n",
    "\n",
    "result = requests.get(api_url, headers=headers, params=params)\n",
    "next_50_cafes = json_normalize(result.json()[\"businesses\"])\n",
    "\n",
    "# Concatenate the results, setting ignore_index to renumber rows\n",
    "cafes = pd.concat([top_50_cafes,next_50_cafes],ignore_index=True)\n",
    "\n",
    "# Print shape of cafes\n",
    "print(cafes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d99a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge crosswalk into cafes on their zip code fields\n",
    "cafes_with_pumas = cafes.merge(crosswalk, left_on=\"location_zip_code\", right_on=\"zipcode\")\n",
    "\n",
    "# Merge pop_data into cafes_with_pumas on puma field\n",
    "cafes_with_pop = cafes_with_pumas.merge(pop_data,on='puma')\n",
    "\n",
    "# View the data\n",
    "print(cafes_with_pop.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI_Projects-tfkgGnhp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
