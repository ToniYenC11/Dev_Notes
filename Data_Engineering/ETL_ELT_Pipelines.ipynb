{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "899dacd5",
   "metadata": {},
   "source": [
    "## Introduction to ETL/ELT Pipelines\n",
    "\n",
    "### ETL\n",
    "- Most common and traditional.\n",
    "\n",
    "### **ELT Pipelines**\n",
    "- used in **data warehouses**. Operated on *tabular data*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced3d781",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(file_name):\n",
    "    print(f\"Extracting data from {file_name}\")\n",
    "    return pd.read_csv(file_name)\n",
    "\n",
    "#* Typical ETL Pipeline\n",
    "# Extract data from the raw_data.csv file\n",
    "extracted_data = extract(file_name=\"raw_data.csv\")\n",
    "\n",
    "# Transform the extracted_data\n",
    "transformed_data = transform(data_frame=extracted_data)\n",
    "\n",
    "# Load the transformed_data to cleaned_data.csv\n",
    "load(data_frame=transformed_data, target_table=\"cleaned_data\")\n",
    "\n",
    "\n",
    "#* Typical ELT Pipeline\n",
    "# Extract data from the raw_data.csv file\n",
    "raw_data = extract(file_name=\"raw_data.csv\")\n",
    "\n",
    "# Load the extracted_data to the raw_data table\n",
    "load(data_frame=raw_data, table_name=\"raw_data\")\n",
    "\n",
    "# Transform data in the raw_data table\n",
    "transform(\n",
    "  source_table=\"raw_data\", \n",
    "  target_table=\"cleaned_data\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c695361",
   "metadata": {},
   "source": [
    "### Another ETL Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0efefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(data_frame, file_name):\n",
    "  # Write cleaned_data to a CSV using file_name\n",
    "  data_frame.to_csv(file_name)\n",
    "  print(f\"Successfully loaded data to {file_name}\")\n",
    "  \n",
    "#* A typical ELT Pipeline\n",
    "extracted_data = extract(file_name=\"raw_data.csv\")\n",
    "\n",
    "# Transform extracted_data using transform() function\n",
    "transformed_data = transform(data_frame=extracted_data)\n",
    "\n",
    "# Load transformed_data to the file transformed_data.csv\n",
    "load(data_frame=transformed_data, file_name=\"transformed_data.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4647f36",
   "metadata": {},
   "source": [
    "### Another ELT Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce550e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete building the transform() function\n",
    "def transform(source_table, target_table):\n",
    "  data_warehouse.execute(f\"\"\"\n",
    "  CREATE TABLE {target_table} AS\n",
    "      SELECT\n",
    "          CONCAT(\"Product ID: \", product_id),\n",
    "          quantity * price\n",
    "      FROM {source_table};\n",
    "  \"\"\")\n",
    "\n",
    "#* Another ELT Pipeline\n",
    "extracted_data = extract(file_name=\"raw_sales_data.csv\")\n",
    "load(data_frame=extracted_data, table_name=\"raw_sales_data\")\n",
    "\n",
    "# Populate total_sales by transforming raw_sales_data\n",
    "transform(source_table=\"raw_sales_data\", target_table=\"total_sales\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c25122",
   "metadata": {},
   "source": [
    "## Extracting Data from Structured Sources\n",
    "\n",
    "- SQL databases\n",
    "- CSV\n",
    "- Parquet\n",
    "- JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7685e22",
   "metadata": {},
   "source": [
    "### Parquet\n",
    "\n",
    "- Data stofare in **Apache Hadoop** system. Similar to *RFile* and *ORC*.\n",
    "- **Columnar data** with efficient representation available to any project in the Hadoop ecosystem.\n",
    "- Used by cloud storages (Amazon S3, Azure Blob Storage, etc.)\n",
    "\n",
    "#### Glossary\n",
    "- **Block (HDFS block)**: This means a block in HDFS and the meaning is unchanged for describing this file format. The file format is designed to work well on top of HDFS.\n",
    "\n",
    "- **File**: A HDFS file that must include the metadata for the file. It does not need to actually contain the data.\n",
    "\n",
    "- **Row group**: A logical horizontal partitioning of the data into rows. There is no physical structure that is guaranteed for a row group. A row group consists of a column chunk for each column in the dataset.\n",
    "\n",
    "- **Column chunk**: A chunk of the data for a particular column. They live in a particular row group and are guaranteed to be contiguous in the file.\n",
    "\n",
    "- **Page**: Column chunks are divided up into pages. A page is conceptually an indivisible unit (in terms of compression and encoding). There can be multiple page types which are interleaved in a column chunk.\n",
    "\n",
    "#### Unit of Parallelization\n",
    "\n",
    "- **MapReduce**  - FIle/Row group\n",
    "- **IO** - column chunk\n",
    "- **Encoding/Compression** - Page\n",
    "\n",
    "#### Relevant Documentations\n",
    "\n",
    "- [File Format](https://parquet.apache.org/docs/file-format/)\n",
    "\n",
    "- [Developer Guide](https://parquet.apache.org/docs/contribution-guidelines/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03605c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#* Reading parquet files\n",
    "raw_stock_data = pd.read_parquet(\"file.parquet\",engine = 'fastparquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40e05be",
   "metadata": {},
   "source": [
    "- Use parquet for **Data Persistence**. For large datasetrs, consider filtering your data into a parquet using `to_parquet`. \n",
    "- Then, load it with `pd.read_parquet`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5693e5f",
   "metadata": {},
   "source": [
    "### Reading SQL Databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786164ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlalchemy\n",
    "import pandas as pd\n",
    "\n",
    "connection_uri = \"postgresql+psycopg2://repl:password@localhost:5432/sales\"\n",
    "db_engine = sqlalchemy.create_engine(connection_uri)\n",
    "raw_stock_data = pd.read_sql(\"SELECT * FROM raw_stock_data LIMIT 10\", db_engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4c773b",
   "metadata": {},
   "source": [
    "### Data Persistence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7723720",
   "metadata": {},
   "source": [
    "#### Utilizing Data Types\n",
    "\n",
    "- Data with only few unique values can be transformed to `Categorical` data type to make it more efficient to read and load.\n",
    "- Use `to_numeric` to downcast numeric columns to their smallest types.\n",
    "\n",
    "#### Chunking\n",
    "\n",
    "- Some workloads can be achieved with chunking by splitting a large problem into a bunch of small problems.\n",
    "-  For example, converting an individual CSV file into a Parquet file and repeating that for each file in a directory.\n",
    "- As long as each chunk fits in memory, you can work with datasets that are much larger than memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea71709c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "N = 12\n",
    "\n",
    "starts = [f\"20{i:>02d}-01-01\" for i in range(N)]\n",
    "\n",
    "ends = [f\"20{i:>02d}-12-13\" for i in range(N)]\n",
    "\n",
    "pathlib.Path(\"data/timeseries\").mkdir(exist_ok=True)\n",
    "\n",
    "for i, (start, end) in enumerate(zip(starts, ends)):\n",
    "    ts = make_timeseries(start=start, end=end, freq=\"1min\", seed=i) # own function\n",
    "    ts.to_parquet(f\"data/timeseries/ts-{i:0>2d}.parquet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2619e865",
   "metadata": {},
   "source": [
    "## Monitoring a Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b524635",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(levelname)s: %(message)s', level=logging.DEBUG)\n",
    "\n",
    "logging.debug(f'variable has value {path}')\n",
    "logging.info('Data transformed now')\n",
    "logging.warning() #warning log\n",
    "logging.error() # error "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e69bff0",
   "metadata": {},
   "source": [
    "## Advanced ETL Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513833d4",
   "metadata": {},
   "source": [
    "### Accessing Non Tabulated Data from JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecabbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the json library\n",
    "import json\n",
    "\n",
    "def extract(file_path):\n",
    "    with open(file_path, \"r\") as json_file:\n",
    "        # Load the data from the JSON file\n",
    "        raw_data = json.load(json_file)\n",
    "    return raw_data\n",
    "\n",
    "raw_testing_scores = extract(\"nested_scores.json\")\n",
    "\n",
    "# Print the raw_testing_scores\n",
    "print(raw_testing_scores)\n",
    "\n",
    "\n",
    "\n",
    "normalized_testing_scores = []\n",
    "\n",
    "# Loop through each of the dictionary key-value pairs\n",
    "for school_id, school_info in raw_testing_scores.items():\n",
    "\tnormalized_testing_scores.append([\n",
    "    \tschool_id,\n",
    "    \tschool_info.get(\"street_address\"),  # Pull the \"street_address\"\n",
    "    \tschool_info.get(\"city\"),\n",
    "    \tschool_info.get(\"scores\").get(\"math\", 0),\n",
    "    \tschool_info.get(\"scores\").get(\"reading\", 0),\n",
    "    \tschool_info.get(\"scores\").get(\"writing\", 0),\n",
    "    ])\n",
    "\n",
    "print(normalized_testing_scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd52028",
   "metadata": {},
   "source": [
    "### Cleaning Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b0736a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(raw_data):\n",
    "\t# Use .loc[] to only return the needed columns\n",
    "\traw_data = raw_data.loc[:, ['city','math_score','reading_score','writing_score']]\n",
    "\t\n",
    "    # Group the data by city, return the grouped DataFrame\n",
    "\tgrouped_data = raw_data.groupby(by=[\"city\"], axis=0).mean()\n",
    "\treturn grouped_data\n",
    "\n",
    "# Transform the data, print the head of the DataFrame\n",
    "grouped_testing_scores = transform(raw_testing_scores)\n",
    "print(grouped_testing_scores.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf2ea2d",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b2d7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(clean_data, con_engine):\n",
    "\t# Store the data in the schools database\n",
    "    clean_data.to_sql(\n",
    "    \tname=\"scores_by_city\",\n",
    "\t\tcon=con_engine,\n",
    "\t\tif_exists=\"replace\",  # Make sure to replace existing data\n",
    "\t\tindex=True,\n",
    "\t\tindex_label=\"school_id\"\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9095191",
   "metadata": {},
   "source": [
    "# Testing a Data Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f8aa78",
   "metadata": {},
   "source": [
    "## Validating Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080e47a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_tax_data = extract(\"raw_tax_data.csv\")\n",
    "clean_tax_data = transform(raw_tax_data)\n",
    "load(clean_tax_data, \"clean_tax_data.parquet\")\n",
    "\n",
    "print(f\"Shape of raw_tax_data: {raw_tax_data.shape}\")\n",
    "print(f\"Shape of clean_tax_data: {clean_tax_data.shape}\")\n",
    "\n",
    "to_validate = pd.read_parquet(\"clean_tax_data.parquet\")\n",
    "print(clean_tax_data.head(3))\n",
    "print(to_validate.head(3))\n",
    "\n",
    "# Check that the DataFrames are equal\n",
    "print(to_validate.equals(clean_tax_data))\n",
    "\n",
    "\n",
    "# Trigger the data pipeline to run three times\n",
    "for attempt in range(0, 3):\n",
    "\tprint(f\"Attempt: {attempt}\")\n",
    "\traw_tax_data = extract(\"raw_tax_data.csv\")\n",
    "\tclean_tax_data = transform(raw_tax_data)\n",
    "\tload(clean_tax_data, \"clean_tax_data.parquet\")\n",
    "\t\n",
    "\t# Print the shape of the cleaned_tax_data DataFrame\n",
    "\tprint(f\"Shape of clean_tax_data: {clean_tax_data.shape}\")\n",
    "    \n",
    "# Read in the loaded data, check the shape\n",
    "to_validate = pd.read_parquet(\"clean_tax_data.parquet\")\n",
    "print(f\"Final shape of cleaned data: {to_validate.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdea4333",
   "metadata": {},
   "source": [
    "## Unit Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1734ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_tax_data = extract(\"raw_tax_data.csv\")\n",
    "clean_tax_data = transform(raw_tax_data)\n",
    "\n",
    "# Validate the number of columns in the DataFrame\n",
    "assert len(clean_tax_data.columns) == 5\n",
    "# Determine if the clean_tax_data DataFrames take type pd.DataFrame\n",
    "isinstance(clean_tax_data, pd.DataFrame)\n",
    "\n",
    "import pytest\n",
    "\n",
    "def test_transformed_data():\n",
    "    raw_tax_data = extract(\"raw_tax_data.csv\")\n",
    "    clean_tax_data = transform(raw_tax_data)\n",
    "    \n",
    "    # Assert that the transform function returns a pd.DataFrame\n",
    "    assert isinstance(clean_tax_data, pd.DataFrame)\n",
    "    \n",
    "    # Assert that the clean_tax_data DataFrame has more columns than the raw_tax_data DataFrame\n",
    "    assert len(clean_tax_data.columns) > len(raw_tax_data.columns)\n",
    "\n",
    "# Create a pytest fixture\n",
    "@pytest.fixture()\n",
    "def clean_tax_data():\n",
    "    raw_data = pd.read_csv(\"raw_tax_data.csv\")\n",
    "    clean_data = transform(raw_data)\n",
    "    return clean_data\n",
    "\n",
    "# Pass the fixture to the function\n",
    "def test_tax_rate(clean_data):\n",
    "    # Assert values are within the expected range\n",
    "    assert clean_tax_data[\"tax_rate\"].max() <= 1 and clean_tax_data[\"tax_rate\"].min() >= 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd7e78f",
   "metadata": {},
   "source": [
    "## Data Pipeline Architecture Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b57bd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from pipeline_utils import extract, transform, load\n",
    "\n",
    "logging.basicConfig(format='%(levelname)s: %(message)s', level=logging.DEBUG)\n",
    "\n",
    "try:\n",
    "\traw_tax_data = extract(\"raw_tax_data.csv\")\n",
    "\tclean_tax_data = transform(raw_tax_data)\n",
    "\tload(clean_tax_data, \"clean_tax_data.parquet\")\n",
    "    \n",
    "\tlogging.info(\"Successfully extracted, transformed and loaded data.\")  # Log a success message.\n",
    "    \n",
    "except Exception as e:\n",
    "\tlogging.error(f\"Pipeline failed with error: {e}\")  # Log failure message"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI_Projects-tfkgGnhp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
